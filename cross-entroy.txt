La cross entropy, teoricamente, si basa sul considerare sostanzialmente le labels e la mia predizione come due PDF (probability density function) ed andare poi a valutare quanto queste due probabilità sono distanti o dissimilari l'una dall'altra.

**Perchè gli addendi della cross entropy corrisponderebbe a valutare la differenza tra due distribuzioni di probabilità?**

Perchè in particolare questa è l'applicazione dell'entropia andando a considerare le due PDF del mio output e della relativa label con una somma di due termini dove il primo si riferisce al caso in cui la random variable assume il valore 1 e l'altra invece in cui assume il valore 0.

Inoltre, quando si verifica un grosso errore tra label e predizione, grazie alla cross entropy è possibile avere un learning più veloce rispetto alla quadratic cost function che invece, quando c'è un grosso errore, ha un rallentamenteo del learning (classico problema dello "slow learning" dovuto al fatto che le derivate parziali rispetto della quadratic cost function wrt W and b sono molto piccole perchè ci ritroviamo a fare la derivata parziale quasi in una flat area e quindi gli updates dei parametri sono molto "lenti"). 
Il comportamento della cross entropy è stato pensato designato così anche per via di come imparano gli umani. In particolare, quando un umano fa un grosso errore, questo impara molto velocemente che quello che ha fatto è un errore. 



Inoltre, la cross entropy è visibile essere una cost function poichè è non negativa.



entropia: può essere interpretato come il livello medio di "informazione", "incertezza" o, se vogliamo, anche "sorpresa" inerente negli output della nostra variable.
